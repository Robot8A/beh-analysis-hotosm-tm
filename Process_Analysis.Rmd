---
title: "Process_Analysis"
output:
  pdf_document: default
  html_notebook: default
---

This is R Notebook present a process analysis of 746 completed, fully validated, and archived projects in the HOT Tasking Manager (HOT-TM) over the past two years.
The process is analised from four perspectives: 

1) [Control flow](#controlflow)
2) [Time](#time)
3) [Organisation](#organisation)
4) [Outcome](#outcome)

Process discovery was performed using bupaR, a suite of open-source R packages for business process data analysis.
```{r}
#install.packages("reshape2")
#install.packages("gt")
library(bupaverse)
library(reshape2)
library(gt)
library(scales)
library(readr)
library(dplyr)
```
# Read event data

The log containing only initial tasks ("initial_tasks.csv") is used in  1) Control flow, 2) Time, 3) Organisation sections, where a clean view of the process is required.
```{r}
event_log_df <- read.csv("initial_tasks.csv", stringsAsFactors = FALSE, sep = ",")
event_log_df <- event_log_df %>%
  convert_timestamps(columns = c("start", "complete"), format = ymd_hms) %>%
  activitylog(case_id = "taskId", activity_id = "action", resource_id = "actionBy", timestamps = c("start", "complete"))
```
```{r}
head(event_log_df)
```

# Control flow {#controlflow}

Absolute frequency of activities in the eventlog.
```{r}
event_log_df %>% activity_frequency("activity")
```
Activity presence shows in what percentage of cases an activity is present. 
```{r}
event_log_df %>% activity_presence()
```
The start of cases can be described using the start_activities function.
```{r}
event_log_df %>% start_activities("activity")
```
The end_activities function describes the end of cases
```{r}
event_log_df %>% end_activities("activity")
```

In the frequency process map, nodes represent the absolute number of activity instance executions and edges represent the absolute number of times source and target activities were executed directly following each other. To provide a clear process map, the event log was previously filter using filter_trace_frequency(). Setting percentage = 0.95 selects at least 95% of the cases, starting with those that have the highest frequency.

```{r}
tmp <- event_log_df %>% filter_trace_frequency(percentage = 0.95)
tmp %>%  process_map(frequency("absolute"))
```

trace_explorer() with coverage argument n_traces = 3 shows the 3 most frequent in the event log.
```{r}
event_log_df %>%    trace_explorer(n_traces = 3, show_labels = FALSE, coverage_labels = c("relative"))
```

# Time {#time}

In the temporal process map, the value of nodes and edges represent the median duration in days of activities and waiting times.
```{r}
tmp1 <- tmp
tmp1 %>%  process_map(performance(median, "days"))
```
Timestamps are properly formatted for time calculations
```{r}
event_log_df <- event_log_df %>% convert_timestamps(columns = c("start", "complete"), format = ymd_hms)
event_log_df$time_diff=as.numeric(event_log_df$complete-event_log_df$start)
task_duration <- event_log_df %>%  group_by(taskId) %>%  summarise(min = min(start), max= max(complete))
task_duration$duration=as.numeric(task_duration$max-task_duration$min)
task_duration <- task_duration[,c("taskId","duration")]
```

We calculated the relative time devoted for mapping activities ('LOCKED_FOR_MAPPING', 'AUTO_UNLOCKED_FOR_MAPPING') and validation activities ('LOCKED_FOR_VALIDATION','AUTO_UNLOCKED_FOR_VALIDATION') per case expressed as percentage of total case duration. The remaining time is considered idle.
```{r}
mapping_duration <- filter(event_log_df, action == 'LOCKED_FOR_MAPPING' | action == 'AUTO_UNLOCKED_FOR_MAPPING') %>%  group_by(taskId) %>% summarise(mapping = sum(time_diff))
validation_duration <- filter(event_log_df, action == 'LOCKED_FOR_VALIDATION' | action == 'AUTO_UNLOCKED_FOR_VALIDATION') %>%  group_by(taskId) %>% summarise(validation = sum(time_diff))
durations <-  merge(x = merge(x = task_duration, y = mapping_duration, by = "taskId", all.x = TRUE), y = validation_duration, by = "taskId", all.x = TRUE)
durations[is.na(durations)] <- 0
durations$mapping_per=durations$mapping/durations$duration*100
durations$validation_per=durations$validation/durations$duration*100
durations$service=durations$mapping_per+durations$validation_per
durations$iddle_per=100-durations$mapping_per-durations$validation_per
```
Median % of iddle time per case
```{r}
median(durations$iddle_per, na.rm = TRUE )
```
Median % of mapping and validation time per case
```{r}
median(durations$service, na.rm = TRUE )
```
Median % of mapping time per case
```{r}
median(durations$mapping_per, na.rm = TRUE )
```
Median % of validation time per case
```{r}
median(durations$validation_per, na.rm = TRUE )
```

# Organisation {#organisation}

The contributor profile is added to the event log to calculate the relative frequency with which contributors according to their mapping level execute each type of activity.
```{r}
contributors <- read.csv("contributors.csv", stringsAsFactors = FALSE, sep = ",")
event_log_df <- merge(event_log_df, contributors, by.x = "actionBy", by.y = "username")
head(event_log_df)
```
Composition of the total number of contributors of the analysed projects according to their mapping level. 
```{r}
mappingLevel <- event_log_df %>%  group_by(mappingLevel) %>% summarise(count = n_distinct(actionBy))
mappingLevel$percentage <- round(mappingLevel$count/sum(mappingLevel$count)*100,1)
mappingLevel
```
Breakdown of status execution frequency per mapping level.
```{r}
data_pivot <- dcast(event_log_df, action ~ mappingLevel,value.var = "taskId", length)
data_pivot$sum <-  data_pivot$ADVANCED + data_pivot$BEGINNER + data_pivot$INTERMEDIATE
data_pivot$ADVANCEDper <- round(data_pivot$ADVANCED/data_pivot$sum*100,1)
data_pivot$BEGINNERper <- round(data_pivot$BEGINNER/data_pivot$sum*100,1)
data_pivot$INTERMEDIATEper <- round(data_pivot$INTERMEDIATE/data_pivot$sum*100,1)
data_pivot[c("action","ADVANCEDper","BEGINNERper","INTERMEDIATEper")] %>% gt() %>%  data_color(columns = 2:4, colors = col_numeric(palette = c("white","darkgreen"),domain = c(0,100)))
```

# Outcome {#outcome}

